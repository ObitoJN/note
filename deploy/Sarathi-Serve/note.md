# Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve
本文聚焦于LLM中吞吐量和延迟的权衡

Sarathi-Serve基于两个关键思想：分块预填充和停滞无调度。

分块预填充将一个预填充请求分割成等计算大小的块，并在多个迭代（每个迭代包含提示标记的一个子集）中计算提示的预填充阶段。停滞无调度允许新请求加入正在运行的批处理，而不会暂停正在进行的解码。这涉及通过将所有正在进行的解码与新请求的一个（或多个）预填充块合并来构建批处理，使得每个批处理达到预配置的块大小。Sarathi-Serve建立在迭代级批处理的基础上，但有一个重要的区别：它在每个迭代中限制预填充标记的数量，同时允许正在运行的批处理中接收新请求。这不仅限制了每个迭代的延迟，还使其几乎不受输入提示总长度的影响。通过这种方式，Sarathi-Serve最小化了计算新预填充对正在进行的解码的TBT（标记间时间）的影响，从而实现了高吞吐量和低TBT延迟。

此外，Sarathi-Serve构建的混合批次（由预填充和解码标记组成）具有近乎统一的计算需求。借助流水线并行性，我们可以创建基于平衡微批处理的调度计划，这可以显著减少流水线气泡并提高GPU利用率，从而实现高效且可扩展的部署。


> 请求级调度的效率很差，先进框架基本采用的是迭代级调度。
> 此外，在迭代调度中，预填充优先和解码优先都是有缺陷的，前者偏向于最大化吞吐，而后者偏向于最小化延迟
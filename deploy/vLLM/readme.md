# Efficient Memory Management for Large Language Model Serving with PagedAttention

## 现有方案
* **现有系统存在内存碎片化问题**。为了在连续空间中存储请求的KV缓存，它们会预先分配一大块内存，按照请求的最大长度（比如2048个词）来分配。这会导致严重的内部碎片化，因为请求的实际长度往往比最大长度短很多。此外，即使实际长度已知，预分配的方式仍然低效：在请求的生命周期内，整个内存块都会被保留，其他较短的请求无法利用当前未使用的部分。而且，不同请求的预分配大小不同，还会导致外部内存碎片化。我们的分析结果显示，现有系统中只有20.4%-38.2%的KV缓存内存实际用于存储标记状态。

* **现有系统无法利用内存共享的机会**。LLM服务常常使用高级解码算法，比如并行采样和束搜索，这些算法会为每个请求生成多个输出。在这些场景中，请求包含的多个序列可以部分共享其KV缓存。然而，现有系统中，序列的KV缓存存储在不同的连续空间中，无法实现内存共享。

> 作者主要对两种方案提出了反对：一种是预分配内存，对于不可预估的生成过程，预分配会造成内存浪费；另一种是压缩存储，在一个对性能敏感的LLM服务系统中执行压缩是不现实的，因为KV缓存非常庞大。即使有压缩，每个请求的预分配块空间也阻碍了现有内存管理系统中特定解码算法的内存共享。

## PagedAttention
PagedAttention将请求的KV缓存分成多个小块(block)，每个小块包含固定数量的注意力键和值。在PagedAttention中，这些小块不需要存储在连续的空间中，因此我们可以像操作系统管理虚拟内存那样灵活地管理KV缓存。通过使用相对较小的小块并按需分配，PagedAttention缓解了内部碎片化问题。

此外，由于所有小块大小相同，它还消除了外部碎片化问题。

最终，它实现了内存共享，使同一请求的不同序列甚至不同请求之间可以在小块级别共享内存。每一个块都有一个引用计数，当计数为0时则淘汰。并且类似操作系统，有一张逻辑内存到物理内存的映射表，更加方便地实现了内存共享。只需要用相同的逻辑地址，就可以直接在不同对话中实现KV Cache共享。

总结来说，PagedAttention有两点优势：
* 减少内存碎片，提高利用率
* 实现KV cache共享，这里的共享包括了在Parallel sampling（多样化输出）、Beam search（最佳输出）、共享前缀这些场景中的KV复用。

## 内存调度
KV cache的驱逐以序列为单位，即一旦驱逐某个序列，就驱逐其所有的KV块
### 交换(swap)
这是大多数虚拟内存实现中使用的经典技术，将被驱逐的页面复制到磁盘上的交换空间。在我们的情况下，我们将被驱逐的块复制到 CPU 内存中。除了 GPU 块分配器外，vLLM 还包括一个 CPU 块分配器，用于管理交换到 CPU RAM 的物理块。当 vLLM 为新令牌耗尽自由物理块时，它选择一组序列进行驱逐，并将它们的 KV 缓存传输到 CPU。一旦抢占了一个序列并驱逐了它的块，vLLM 就停止接受新请求，直到所有抢占的序列完成为止。一旦一个请求完成，其块就会从内存中释放，抢占的序列的块就会被重新引入以继续处理该序列。请注意，通过这种设计，交换到 CPU RAM 的块数量永远不会超过 GPU RAM 中为 KV 缓存分配的物理块的数量，因此 CPU RAM 上的交换空间受到 GPU 内存的限制。
### 重新计算
在这种情况下，当抢占的序列被重新调度时，我们简单地重新计算 KV 缓存。请注意，重新计算的延迟可能明显低于原始延迟，因为在解码时生成的令牌可以与原始用户提示连接在一起，它们在所有位置的 KV 缓存可以在一个提示阶段迭代中生成。

交换和重新计算的性能取决于 CPU RAM 和 GPU 内存之间的带宽以及 GPU 的计算能力。vLLM支持重新计算和交换作为其恢复机制。为了了解两种方法之间的权衡，我们评估它们的端到端性能，并对它们的开销进行微基准测试，如图19所示。我们的结果显示，交换在小块大小时产生了过多的开销。这是因为小块大小通常导致CPU和GPU之间大量的小数据传输，从而限制了有效的PCIe带宽。相比之下，重新计算的开销在不同的块大小下保持不变，因为重新计算不利用KV块。因此，当块大小较小时，重新计算更有效率，而当块大小较大时，交换更有效率，尽管重新计算的开销从未高于交换的延迟的20%。



  
